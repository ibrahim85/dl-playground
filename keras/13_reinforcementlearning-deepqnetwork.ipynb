{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Verify system(python intepreter) and python version number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3 |Continuum Analytics, Inc.| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys #provide objects/functions interact with intepretwer\n",
    "print (sys.version) # python 2.7 is needed to run Keras Reinforcement Learning(Environment OpenAIGYM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magic set up matplotlib to work interactively(within Jnote)\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Load dependencies(interactive data science and machine learning-Keras reinforcement learning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #provides a MATLAB like plotting framework(Jnote)\n",
    "import numpy as np #provide computing array(carry to tensorflow) format \n",
    "import pandas as pd #provide data analysis and manipulation Python library\n",
    "import seaborn as sns #provide statistical date visualization(linear+distribution+timeseries)\n",
    "sns.set() #set seaborn format(font, color, axes styple) in one step\n",
    "import keras #provide tensorflow and framework(17 Keras libraries)\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential #provide linear stack of layers to create neural network\n",
    "from keras.layers import Dense #provide dot product function,  output = activation(dot(input, kernel) + bias)` \n",
    "from keras.layers import Activation #provide activation(etc, RELU) function to the output layer \n",
    "from keras.layers import Flatten #flatten the input(etc, 60,60,3) becomes 10,800\n",
    "from keras.optimizers import Adam #provide Adam optimzer(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, **kwargs)\n",
    "from rl.agents.dqn import DQNAgent #take in agent policy, provides compile(take in optimizer, metric) function\n",
    "from rl.policy import BoltzmannQPolicy # available options for EpsGreedyPolicy\n",
    "from rl.memory import SequentialMemory #provide data ring bufferbuffer rather than deque data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build environment(openai gym Cartpole) and neural network(3 dense layers- 16 neurons/layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-20 16:57:08,495] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658.0\n",
      "Trainable params: 658.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0' # Acrobot-v1, MountainCar-v0, is functional in Win10(supports python35 upwards)\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)  #provide the environment with step, reset, render\n",
    "np.random.seed(123)\n",
    "env.seed(123) #sets the the seed for this env's random number generator\n",
    "nb_actions = env.action_space.n # equal nb_actions with the environment action state(etc cart pole has 2 states, move horizontally left or right)\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network using Deep Q Network learning with Boltzmann policy, and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 steps ...\n",
      "done, took 0.368 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1) #set memory for network training \n",
    "policy = BoltzmannQPolicy() #set the policy, agent takes in default(EpsGreedy) as the default policy \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "#dqn.load_weights('dqn_cartPole-v0_weights.h5f')\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=10, visualize=True, verbose=2) #take in environment, number of training step-1 training steps takes average 0.3sec, turn on \n",
    "#training visualization\n",
    "\n",
    "# After training is done, we save the final weights\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format('CartPole-weights-after-100actions'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Deep Q Network with results(rewards) over 5 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 58.000, steps: 58\n",
      "Episode 2: reward: 25.000, steps: 25\n",
      "Episode 3: reward: 149.000, steps: 149\n",
      "Episode 4: reward: 20.000, steps: 20\n",
      "Episode 5: reward: 86.000, steps: 86\n"
     ]
    }
   ],
   "source": [
    "result = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
