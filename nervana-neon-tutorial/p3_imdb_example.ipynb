{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial 2: Sentiment analysis with a recurrent LSTM network\n",
    "=============================================================\n",
    "\n",
    "Train an recurrent neural network to parse movie reviews from IMDB and decide if they are positive or negative reviews.\n",
    "\n",
    "Data preprocessing:\n",
    "\n",
    "* Converting words to one-hot\n",
    "  * Top 20,000 words\n",
    "  * PAD, OOV, START tags\n",
    "  * ID's based on frequency\n",
    "* Pre-defined sentence length\n",
    "* Targets binarized to positive (>=7), negative (<7)\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "* Embedding layer: Learning to embed words from a sparse representation to a dense space\n",
    "* LSTM layer: Recurrent layer that learns features in the time series\n",
    "* Recurrent sum layer: Go from time series to single output\n",
    "* Affine layer: Readout / classfier\n",
    "\n",
    "An example of a review:\n",
    "\n",
    "`\"Okay, sorry, but I loved this movie. I just love the whole 80's genre of these kind of movies, because you don't see many like this ...\n",
    "~*~CupidGrl~*~\"`\n",
    "\n",
    "We have 25000 reviews like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preparation\n",
    "-------------------\n",
    "We have a script that takes reviews from a text file and store as one-hot encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read reviews from text file and store as one-hot encoded dataset\n",
    "import prepare\n",
    "fname='labeledTrainData.tsv'\n",
    "prepare.main(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Building the model\n",
    "---------------------\n",
    "Similar to the convnet example, we need dataset, layers, callbacks, backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 128\n",
    "embedding_dim = 128\n",
    "vocab_size = 20000\n",
    "sentence_length = 128\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "\n",
    "# setup backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend(backend='cpu',\n",
    "                 batch_size=batch_size,\n",
    "                 rng_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the h5 datasets, print stats\n",
    "import h5py\n",
    "h5f = h5py.File(fname + '.h5', 'r')\n",
    "reviews, h5train, h5valid = h5f['reviews'], h5f['train'], h5f['valid']\n",
    "ntrain, nvalid, nclass = reviews.attrs['ntrain'], reviews.attrs['nvalid'], reviews.attrs['nclass']\n",
    "print \"# of train examples - {0}, valid examples - {1}\".format(ntrain, nvalid)\n",
    "print \"# of classes - \", nclass\n",
    "print \"class distribution - \", reviews.attrs['class_distribution']\n",
    "print \"vocab size - {0}, sentence_length - {1}\".format(vocab_size, sentence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datsets\n",
    "* Split data into a training and validation set.\n",
    "* Pad / truncate reviews to 128 words. \n",
    "* Finally wrap them into a DataIterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make train dataset\n",
    "from preprocess_text import get_paddedXY\n",
    "from neon.data import DataIterator\n",
    "Xy = h5train[:ntrain]\n",
    "X = [xy[1:] for xy in Xy]\n",
    "y = [xy[0] for xy in Xy]\n",
    "X_train, y_train = get_paddedXY(\n",
    "    X, y, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "train_set = DataIterator(X_train, y_train, nclass=nclass)\n",
    "\n",
    "# make valid dataset\n",
    "Xy = h5valid[:nvalid]\n",
    "X = [xy[1:] for xy in Xy]\n",
    "y = [xy[0] for xy in Xy]\n",
    "X_valid, y_valid = get_paddedXY(\n",
    "    X, y, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "valid_set = DataIterator(X_valid, y_valid, nclass=nclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializers\n",
    "\n",
    "We use Xavier Glorot's initialization scheme to automatically scale the weights to preserve the variance of input activations on the output side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "from neon.initializers import GlorotUniform, Uniform\n",
    "init_glorot = GlorotUniform()\n",
    "init_emb = Uniform(-0.1 / embedding_dim, 0.1 / embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model layers\n",
    "The network consists of a word embedding layer, and LSTM, a RecurrentSum, Dropout and an Affine layer.\n",
    "* **LookupTable** is a word embedding that maps from a sparse one-hot representation to dense word vectors. The embedding is learned from the data.\n",
    "* **LSTM** is a recurrent layer with \"long short-term memory\" units. LSTM networks tend to be easier to train, and perform similar to standard RNN layers.\n",
    "* **RecurrentSum** is a recurrent output layer that collapeses over the time dimension of the sequence by summing up outputs from individual steps.\n",
    "* **Dropout** performs regularizaion by randomly zeroing out some of the units.\n",
    "* **Affine** is a fully connected MLP layer that is used for the binary classification of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define layers\n",
    "from neon.layers import LookupTable, LSTM, RecurrentSum, Dropout, Affine\n",
    "from neon.transforms import Softmax, Tanh, Logistic\n",
    "layers = [\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(nclass, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Optimizer\n",
    "Use a cross-entropy cost function and an Adagrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the cost, metrics, optimizer\n",
    "from neon.layers import GeneralizedCost\n",
    "from neon.transforms import CrossEntropyMulti, Accuracy\n",
    "from neon.models import Model\n",
    "from neon.optimizers import Adagrad\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\n",
    "metric = Accuracy()\n",
    "model = Model(layers=layers)\n",
    "optimizer = Adagrad(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "In addition to the default progress bar, we set up a callback to save the model to a pickle file after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure callbacks\n",
    "from neon.callbacks import Callbacks\n",
    "callbacks = Callbacks(model, train_set, eval_set=valid_set, \n",
    "                      epochs=num_epochs, serialize=1,\n",
    "                      save_path=fname + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We now have all the parts in place to train the model. Two epochs are sufficient to obtain some interesting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model.fit(train_set, optimizer=optimizer, num_epochs=num_epochs,\n",
    "          cost=cost, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on test and valiadation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pct = 100 * model.eval(valid_set, metric=metric)[0]\n",
    "train_pct = 100 * model.eval(train_set, metric=metric)[0]\n",
    "\n",
    "print \"Test Accuracy: %2.1f%%\" % test_pct\n",
    "print \"Train Accuracy: %2.1f%%\" % train_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. Inference\n",
    "------------\n",
    "The trained model can now be used to perform inference on new reviews. Set up a new model with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend(batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a new set of layers for batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define same model as in train. Layers need to be recreated with new batch size. \n",
    "layers = [\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(nclass, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warp the new layers into a new model, initialize with the weights we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_new = Model(layers=layers)\n",
    "\n",
    "# load the weights\n",
    "save_path= 'labeledTrainData.tsv' + '.pickle'\n",
    "model_new.load_weights(save_path)\n",
    "model_new.initialize(dataset=(sentence_length, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try in on some real reviews!\n",
    "\n",
    "I went on [imdb](http://www.imdb.com/title/tt2379713/reviews?ref_=tt_ov_rt) to get some reviews of the latest Bond Movie.\n",
    "\n",
    "*As a die hard fan of James Bond, I found this film to be simply nothing more than a classic. For any original James Bond fan, you will simply enjoy how the producers and Sam Mendes re-emerged the roots of James Bond. The roots of Spectre, Blofield and just the pure elements of James Bond that we all miss even from the gun barrel introduction. This film deserves higher ratings in my view. I don't want to spoil the film , but I am finally glad the writers brought back the roots of James Bond. A true fan nothing more nothing less. I don't know what else to expect from a James bond film and Spectre does just what I originally expected in a James Bond film. It opens a whole new extension to have many more films to come. The cast does a superb in their roles and many salutes to Christopher Waltz in his enemy role.*\n",
    "\n",
    "and another one\n",
    "\n",
    "*The plot/writing is completely unrealistic and just dumb at times. Bond is dressed up in a white tux on an overnight train ride? eh, OK. But then they just show up at the villain's compound like nothing bad is going to happen to them. How stupid is this Bond? And then the villain just happens to booby trap this huge building in London (across from the intelligence building) and previously or very quickly had some bullet proof glass installed.*\n",
    "\n",
    "*And so on and so on... give me a break. And then there was the terrible credit sequence at the beginning that was hell bent on turning Daniel Craig into a sex object. I don't mind that, but when you show him in the credit sequence with his shirt off there isn't even the pretense of something else going on. They were trying way too hard.*\n",
    "\n",
    "*There was some of the same writers as the previous newer (Craig) Bonds too, and I enjoyed those. Someone must have come along and thought they had some great ideas. That person should be fired.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import preprocess_text\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "# setup buffers before accepting reviews\n",
    "xbuf = np.zeros((sentence_length, 1), dtype=np.int32)  # host buffer\n",
    "xdev = be.zeros((sentence_length, 1), dtype=np.int32)  # device buffer\n",
    "\n",
    "# tags for text pre-processing\n",
    "oov = 2\n",
    "start = 1\n",
    "index_from = 3\n",
    "pad_char = 0\n",
    "\n",
    "# load dictionary from file (generated by prepare script)\n",
    "vocab, rev_vocab = cPickle.load(open(fname + '.vocab', 'rb'))\n",
    "\n",
    "while True:\n",
    "    line = raw_input('Enter a Review from testData.tsv file: \\n')\n",
    "\n",
    "    # clean the input\n",
    "    tokens = preprocess_text.clean_string(line).strip().split()\n",
    "\n",
    "    # convert strings to one-hot. Check for oov and add start\n",
    "    sent = [len(vocab) + 1 if t not in vocab else vocab[t] for t in tokens]\n",
    "    sent = [start] + [w + index_from for w in sent]\n",
    "    sent = [oov if w >= vocab_size else w for w in sent]\n",
    "\n",
    "    # pad sentences\n",
    "    xbuf[:] = 0\n",
    "    trunc = sent[-sentence_length:]\n",
    "    xbuf[-len(trunc):, 0] = trunc  # load list into numpy array\n",
    "    xdev[:] = xbuf  # load numpy array into device tensor\n",
    "    \n",
    "    # run the sentence through the model\n",
    "    y_pred = model_new.fprop(xdev, inference=True)\n",
    "    \n",
    "    print '-' * 100\n",
    "    print \"Sentence encoding: {0}\".format(xbuf.T)\n",
    "    print \"\\nPrediction: {:.1%} negative, {:.1%} positive\".format(y_pred.get()[0,0], y_pred.get()[1,0])\n",
    "    print '-' * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
